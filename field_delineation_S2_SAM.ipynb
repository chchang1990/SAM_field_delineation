{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPjdl9c4dcTY41eO2In3Jud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chchang1990/SAM_field_delineation/blob/main/field_delineation_S2_SAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delineating Crop Field Boundaries from Sentinel-2 (S2) imagery with Segment-Anything Model (SAM)\n",
        "\n",
        "**GPU runtime is suggested to run this script.**\n",
        "\n",
        "This script delineates the crop field boundaries on S2 imagery by using the <i>segment-geospatial</i> Python package developed by Prof. Qiusheng Wu at the University of Tennessee, Knoxville. The <i>segment-geospatial</i> package utilizes Meta's Segment-Anything Model (SAM) to generate segmented masks of different features on a remote sensing imagery as separated polygons. Hence, theoretically, if SAM is applied over the croplands, the boundaries of the segmented polygons can be considered as the crop field boundaries.\n",
        "\n",
        "\n",
        "This script attempts to answer two questions:    \n",
        "1. If SAM can really be used to delineate the crop field boundaries?\n",
        "\n",
        "2. Since false-color image enhance the contrast between crops that have different health conditions, can SAM reveal the boundaries between crops that have different health conditions if it is applied to false-color image?\n",
        "\n",
        "To answer these two question, I retrieved both S2 true-color and false-color images from the Google Earth Engine, then delineated the crop field boundaries using SAM. The results were compared with the USDA Cropland Data Layer, as well as the coincident S2 NDVI image.\n",
        "\n",
        "The results showed that the boundaries delineated from both types of imagery generally agree with the boundaries shown on the USDA Cropland Data Layer. This indicates that SAM can be used as a tool that quickly delineate the field boundaries.\n",
        "\n",
        "In addition, the field boundaries delineated from the false-color image seem to better reveal some details shown on the NDVI image, indicating that applying SAM on the false-color image may be used to show the boundaries between crops that have different health conditions. However, more examination on the delineated results is required to reach a solid conclusion.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TwjefT_KHcHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the required packages"
      ],
      "metadata": {
        "id": "-GmnIKYhIAbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import the packages required to run this script."
      ],
      "metadata": {
        "id": "aJVM8_qaLi65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geemap rioxarray segment-geospatial"
      ],
      "metadata": {
        "id": "_WotG_BJHcYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import leafmap\n",
        "from samgeo import SamGeo, tms_to_geotiff, get_basemaps\n",
        "\n",
        "import ee\n",
        "import geemap\n",
        "import google.colab.drive as drive"
      ],
      "metadata": {
        "id": "NEhDpeGDHca7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare S2 images to be used for crop delineation"
      ],
      "metadata": {
        "id": "zMOzzkRwNnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Google Earth Engine API and Google Drive ready"
      ],
      "metadata": {
        "id": "Z7fZPobclQnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we will be retrieving the S2 images from the Google Earth Engine, first of all we need to authenticate and initialize the Google Earth Engine API so that we can use it in this Colab script."
      ],
      "metadata": {
        "id": "Ws-MsCSJK11p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "metadata": {
        "id": "RltDMO8KK1_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's mount our Google Drive to the Google Colab for the convenience of data export and access."
      ],
      "metadata": {
        "id": "boMKA3O8k3bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')\n",
        "root_folder = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "T1bPJEkzK2CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an Area-Of-Interest (AOI)\n",
        "\n",
        "An AOI is where the retrieved S2 images will cover. We will be performing delineation for the crop fields inside the AOI as well.\n",
        "\n",
        "In this script, I defined a small AOI in Iowa state, the state that has the highest average percentage of corn and soybean productions in the U.S."
      ],
      "metadata": {
        "id": "jzA7ZSLTlfSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the AOI with a bounding box\n",
        "sample_aoi_w, sample_aoi_s, sample_aoi_e, sample_aoi_n = -94., 43.25, -93.9, 43.35\n",
        "sample_aoi = ee.Geometry.BBox(sample_aoi_w, sample_aoi_s, sample_aoi_e, sample_aoi_n)\n",
        "\n",
        "# Visualize the AOI.\n",
        "Map = geemap.Map(center=((sample_aoi_n+sample_aoi_s)/2,(sample_aoi_w+sample_aoi_e)/2), zoom=12)\n",
        "Map.addLayer(sample_aoi,{},name='Sample AOI')\n",
        "Map.addLayerControl()\n",
        "Map"
      ],
      "metadata": {
        "id": "lIoOdg5UK2FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve S2 composite images from the Google Earth Engine"
      ],
      "metadata": {
        "id": "mTfXzfRJRDtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's go ahead and retrieve S2 true-color and false-color images from the Google Earth Engine."
      ],
      "metadata": {
        "id": "9BIjLxGHoS7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def s2_composite(doi, sample_aoi, band_list):\n",
        "\n",
        "    \"\"\"\n",
        "    This is a mapping function that retrieve S2 image collection of defined bands at\n",
        "    the given acquisition time inside an AOI.\n",
        "    \"\"\"\n",
        "\n",
        "    # A mapping function for cloud masking and value scaling\n",
        "    def maskS2clouds(image):\n",
        "        qa = image.select('QA60');\n",
        "\n",
        "        # QA has 12 bits from bit-0 to bit-11\n",
        "        # Bits 10 and 11 are clouds and cirrus, respectively.\n",
        "        cloudBitMask = 1 << 10; # Push \"1\" 10 spaces to the left (010000000000)\n",
        "        cirrusBitMask = 1 << 11; # Push \"1\" 11 spaces to the left (100000000000)\n",
        "\n",
        "        # \"bitwiseAnd\" compare the QA and \"cloudBitMask\" and \"cirrusBitMask\"\n",
        "        # then return True if (1) bit-10 of QA and \"cloudBitMask\" do not match (no cloud)\n",
        "        #                     (2) bit-11 of QA and \"cirrusBitMask\" do not match (no cirrus cloud)\n",
        "        #\n",
        "        # Both flags should be set to zero, indicating clear conditions.\n",
        "        # (a pixel that is neither cloud nor cirrus cloud will be retained)\n",
        "        mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0));\n",
        "        #\n",
        "        # Mask out the cloud/cirrus-cloud pixels and scale the data by 10000 (the scale factor of the data)\n",
        "        return image.updateMask(mask).divide(10000).copyProperties(image, ['system:time_start'])\n",
        "\n",
        "    # A mapping function to scale the composited images to 0-255 range\n",
        "    def rgb_uint8(image):\n",
        "        return image.multiply(255).uint8().copyProperties(image, ['system:time_start'])\n",
        "\n",
        "    # A mapping function to clip the image collection to the defined AOI\n",
        "    def imgcol_clip(image):\n",
        "        return image.clip(sample_aoi)\n",
        "\n",
        "\n",
        "    doi_end = ee.Date(doi).advance(1,'day').format('YYYY-MM-dd')\n",
        "\n",
        "    # Retrieve the selected bands of S2 image collection\n",
        "    color_outfname = 's2_'+''.join(band_list)+'_'+doi\n",
        "    s2_composite_ImgCol = (\n",
        "        ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "        .filterDate(doi, doi_end)\n",
        "        ##Pre-filter to get less cloudy granules.\n",
        "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',20))\n",
        "        .filterBounds(sample_aoi)\n",
        "    ).map(maskS2clouds).select(band_list).map(rgb_uint8).map(imgcol_clip)\n",
        "\n",
        "    return s2_composite_ImgCol, color_outfname\n"
      ],
      "metadata": {
        "id": "fA3NG-28jIz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The true-color images are nothing special but just visualizing the Red, Green, and Blue band reflectance as Red, Green, and Blue as they are.\n",
        "\n",
        "For the false-color images, I used the most common band combination, which visualizes Near-Infrared (NIR), Red, and Green band reflectances as the Red, Green, and Blue on the images, repsectively. This band combination better reveals the health condition of crops than the true color images. Since healthier crops have much stronger NIR reflectances which are visualized as red, healthier crops will appear in bright red on the false-color images. On the other hand, the less healthier crops or bare ground will appear in tan."
      ],
      "metadata": {
        "id": "mDs_dfSorbm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I arbitrarily selected a Date-Of-Interest (DOI) on 2022-07-13 to get S2 true-color and false-color images. According to USDA, July is the growing season of both corn and soybean (https://ipad.fas.usda.gov/countrysummary/Default.aspx?id=US&crop=Corn).\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1-6gE5fC5Fa3nC7m-GJT0_5wrGjGq2RLR)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1-E3szEBN1Ox77OLrCWiHHGfJGv3LNssq)\n",
        "\n",
        "\n",
        "Since there may have several tiles of image in the AOI on the DOI, I only used the earliest acquired image on the DOI here."
      ],
      "metadata": {
        "id": "KIqtOoB5se-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DOI, and true-color, and false-color band lists.\n",
        "doi = '2022-07-13'\n",
        "\n",
        "true_color_bands = ['B4','B3','B2']\n",
        "false_color_bands = ['B8','B4','B3']\n",
        "\n",
        "# Retreive S2 true-color and false-color images from the Google Earth Engine\n",
        "true_color_s2_ImgCol, true_color_s2_outfname = s2_composite(doi, sample_aoi, true_color_bands)\n",
        "false_color_s2_ImgCol, false_color_s2_outfname = s2_composite(doi, sample_aoi, false_color_bands)\n",
        "\n",
        "# Get the earliest-acquired image on the DOI\n",
        "true_color_s2_Img = true_color_s2_ImgCol.first()\n",
        "false_color_s2_Img = false_color_s2_ImgCol.first()"
      ],
      "metadata": {
        "id": "E2o5qPDhRHxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize retrieved S2 true-color and false-color images\n",
        "Map_Img = geemap.Map(center=((sample_aoi_n+sample_aoi_s)/2,(sample_aoi_w+sample_aoi_e)/2), zoom=12)\n",
        "Map_Img.addLayer(true_color_s2_Img, {'min':0,'max':255,'bands':true_color_bands, 'gamma':1.7}, name=true_color_s2_outfname)\n",
        "Map_Img.addLayer(false_color_s2_Img, {'min':0,'max':255,'bands':false_color_bands, 'gamma':1.7}, name=false_color_s2_outfname)\n",
        "Map_Img.addLayerControl()\n",
        "Map_Img"
      ],
      "metadata": {
        "id": "819iSxktSfJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IzLcMLQhqUyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the <i>segment-geospatial</i> package use GeoTIFF as input, now let's export the S2 true-color and false-color images that we just retrieved as GeoTIFFs to our Google Drive."
      ],
      "metadata": {
        "id": "h10X8PV6rIXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_eeImg(ee_image, export_folder, export_filename, scale=10):\n",
        "    \"\"\"\n",
        "    A function that exports an Earth Engine image to Google Drive.\n",
        "    \"scale\" is the export spatial resolution in meter.\n",
        "    \"\"\"\n",
        "\n",
        "    task = ee.batch.Export.image.toDrive(**{\n",
        "        'image': ee_image,\n",
        "        'description': export_filename,\n",
        "        'folder':export_folder,\n",
        "        'scale': scale,\n",
        "        'region': ee_image.geometry().getInfo()['coordinates']\n",
        "    })\n",
        "    task.start()"
      ],
      "metadata": {
        "id": "eetK-UznrHxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder in Google Drive that you want to export the image to\n",
        "export_folder = 'sentinel2_composite'\n",
        "\n",
        "# Export the image\n",
        "export_eeImg(true_color_s2_Img, export_folder, true_color_s2_outfname, scale=10)\n",
        "export_eeImg(false_color_s2_Img, export_folder, false_color_s2_outfname, scale=10)\n"
      ],
      "metadata": {
        "id": "pw9uINLyqU82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crop field delineation from S2 images using SAM\n",
        "\n",
        "Now that we have the S2 true-color and false-color images ready, let's delineate the crop field boundary."
      ],
      "metadata": {
        "id": "dD4Ndu-IWJSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM segmentation"
      ],
      "metadata": {
        "id": "Q5pYbVIu5hlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sam_segment(input_image_path, output_folder):\n",
        "\n",
        "    \"\"\"\n",
        "    A wrap-up function that applying SAM to segment the crop field into polygons.\n",
        "    The boundaries of polygons are considered as crop field boundaries\n",
        "    \"\"\"\n",
        "\n",
        "    output_segment_path = output_folder+input_image_path.split('/')[-1][:-4]+'_delineation.tif'\n",
        "\n",
        "    sam.generate(\n",
        "        input_image_path, output_segment_path, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n",
        "    )\n",
        "\n",
        "    output_segment_vector_path = output_segment_path[:-4]+\".gpkg\"\n",
        "    output_segment_shp_path = output_segment_path[:-4]+\".shp\"\n",
        "    sam.tiff_to_gpkg(output_segment_path, output_segment_vector_path, simplify_tolerance=None)\n",
        "    sam.tiff_to_vector(output_segment_path, output_segment_shp_path)\n",
        "\n",
        "    return output_segment_vector_path\n"
      ],
      "metadata": {
        "id": "9tqstairSF5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SAM modeol\n",
        "sam = SamGeo(\n",
        "    model_type=\"vit_h\",\n",
        "    checkpoint=\"sam_vit_h_4b8939.pth\",\n",
        "    sam_kwargs=None,\n",
        ")"
      ],
      "metadata": {
        "id": "8eoxRA6ASfQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#root_folder = \"/content/drive/MyDrive/\"\n",
        "#export_folder = \"sentinel2_composite\"\n",
        "\n",
        "input_image_folder = root_folder+export_folder+\"/\"\n",
        "sam_segment_outfolder = input_image_folder+\"sam/\"\n",
        "if not os.path.exists(sam_segment_outfolder):\n",
        "    os.makedirs(sam_segment_outfolder)\n",
        "\n",
        "\n",
        "# If you run into error saying the image does not exist, please check if two images were\n",
        "# exported to different folders in Google Drive. Earth Engine API creates a renamed folder if it is already exists.\n",
        "true_color_image_path = input_image_folder+true_color_s2_outfname+\".tif\"\n",
        "true_color_sam_vector_path = sam_segment(true_color_image_path, sam_segment_outfolder)\n",
        "\n",
        "false_color_image_path = input_image_folder+false_color_s2_outfname+\".tif\"\n",
        "false_color_sam_vector_path = sam_segment(false_color_image_path, sam_segment_outfolder)"
      ],
      "metadata": {
        "id": "GJ-yd2nUSV1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the delineated crop field boundaries\n",
        "\n",
        "Now we have successfully segmented the S2 true-color and false-color images into polygons, whose boundaries can be considered as crop field boundaries. Let's visualize them!\n",
        "\n",
        "Since I am also curious if using false-color images for crop field delineation can better differentiate crop fields with different health conditions, I also calculated the Normalized Difference Vegetation Index (NDVI) as reference for comparison.\n",
        "\n",
        "Simply put, NDVI is the normalized difference of NIR and red band reflectances. Since healthy crops reflect lots of NIR and absorb more red light for photosynthesis process and creating chlorophyll, higher NDVI indicating healthier crops."
      ],
      "metadata": {
        "id": "gMuKTviMvoGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def s2_vi(doi, vi_type, sample_aoi):\n",
        "\n",
        "    \"\"\"\n",
        "    A wrap-up function that retrieves and calculates S2 vegetaion indices (e.g., NDVI, LSWI)\n",
        "    \"\"\"\n",
        "\n",
        "    def maskS2clouds(image):\n",
        "        qa = image.select('QA60');\n",
        "\n",
        "        # QA has 12 bits from bit-0 to bit-11\n",
        "        # Bits 10 and 11 are clouds and cirrus, respectively.\n",
        "        cloudBitMask = 1 << 10; # Push \"1\" 10 spaces to the left (010000000000)\n",
        "        cirrusBitMask = 1 << 11; # Push \"1\" 11 spaces to the left (100000000000)\n",
        "\n",
        "        # \"bitwiseAnd\" compare the QA and \"cloudBitMask\" and \"cirrusBitMask\"\n",
        "        # then return True if (1) bit-10 of QA and \"cloudBitMask\" do not match (no cloud)\n",
        "        #                     (2) bit-11 of QA and \"cirrusBitMask\" do not match (no cirrus cloud)\n",
        "        #\n",
        "        # Both flags should be set to zero, indicating clear conditions.\n",
        "        # (a pixel that is neither cloud nor cirrus cloud will be retained)\n",
        "        mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0));\n",
        "        #\n",
        "        # Mask out the cloud/cirrus-cloud pixels and scale the data by 10000 (the scale factor of the data)\n",
        "        return image.updateMask(mask).divide(10000).copyProperties(image, ['system:time_start'])\n",
        "\n",
        "    def imgcol_clip(image):\n",
        "        return image.clip(sample_aoi)\n",
        "\n",
        "\n",
        "    def cal_lswi(image):\n",
        "        lswi = image.expression(\n",
        "            \"(NIR - SWIR)/(NIR + SWIR)\",\n",
        "            {\n",
        "                \"NIR\": image.select('B8'),\n",
        "                \"SWIR\": image.select('B11')\n",
        "            }\n",
        "        ).rename('LSWI').copyProperties(image, ['system:time_start']);\n",
        "        #image = image.addBands(ndvi)\n",
        "\n",
        "        return lswi\n",
        "\n",
        "\n",
        "    def cal_ndvi(image):\n",
        "        ndvi = image.expression(\n",
        "            \"(NIR - RED)/(NIR + RED)\",\n",
        "            {\n",
        "                \"NIR\": image.select('B8'),\n",
        "                \"RED\": image.select('B4')\n",
        "            }\n",
        "        ).rename('NDVI').copyProperties(image, ['system:time_start']);\n",
        "        #image = image.addBands(ndvi)\n",
        "\n",
        "        return ndvi\n",
        "\n",
        "    doi_end = ee.Date(doi).advance(1,'day').format('YYYY-MM-dd')\n",
        "\n",
        "    # Retrieve S2 images with selected bands\n",
        "    color_outfname = 's2_'+vi_type+'_'+doi\n",
        "    s2_ImgCol = (\n",
        "        ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "        .filterDate(doi, doi_end)\n",
        "        ##Pre-filter to get less cloudy granules.\n",
        "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',20))\n",
        "        .filterBounds(sample_aoi)\n",
        "    ).map(maskS2clouds)\n",
        "\n",
        "    if vi_type == 'lswi':\n",
        "        vi_ImgCol = s2_ImgCol.map(cal_lswi).map(imgcol_clip)\n",
        "    elif vi_type == 'ndvi':\n",
        "        vi_ImgCol = s2_ImgCol.map(cal_ndvi).map(imgcol_clip)\n",
        "\n",
        "    return vi_ImgCol, color_outfname"
      ],
      "metadata": {
        "id": "NBQIVaSEKoU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_img_vec(map_obj, ee_img, band_list, segment_vector, ee_img_name, style, gamma_value=2., plot_image_opt=False):\n",
        "\n",
        "    \"\"\"\n",
        "    A wrap-up function for visualizing S2 images and the correpsonding crop field boundaries.\n",
        "    \"\"\"\n",
        "    if plot_image_opt==True:\n",
        "        map_obj.addLayer(ee_img, {'min':0,'max':255,'bands':band_list,'gamma':gamma_value}, name=ee_img_name)\n",
        "\n",
        "    map_obj.add_vector(segment_vector, layer_name=ee_img_name+'_delineation', style=style)\n"
      ],
      "metadata": {
        "id": "UuX2mNaSvng-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DOI, it has to be the same as the crop field boundaries that we just\n",
        "# delineated from S2 images\n",
        "#doi = '2022-07-13'\n",
        "\n",
        "# Calculate the S2 NDVI in the AOI on DOI\n",
        "ndvi_s2_ImgCol, ndvi_s2_outfname = s2_vi(doi, 'ndvi', sample_aoi)\n",
        "\n",
        "# Get the earliest-acquired image on DOI\n",
        "ndvi_s2_Img = ndvi_s2_ImgCol.first()\n"
      ],
      "metadata": {
        "id": "vLtMeoTNKsE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Style for visualizing the segmented polygons\n",
        "style = {\n",
        "    \"color\": \"#3388ff\",\n",
        "    \"weight\": 2,\n",
        "    \"fillColor\": \"#7c4185\",\n",
        "    \"fillOpacity\": 0.,\n",
        "}\n",
        "\n",
        "# Retrieve USDA Cropland data as background\n",
        "usda_crop = (\n",
        "    ee.ImageCollection(\"USDA/NASS/CDL\")\n",
        "    .filterDate('2022-01-01','2022-12-31')\n",
        "    .first()\n",
        "    .select('cropland')\n",
        ") .clip(sample_aoi)\n",
        "\n",
        "\n",
        "Map_ImgSeg = geemap.Map(center=((sample_aoi_n+sample_aoi_s)/2,(sample_aoi_w+sample_aoi_e)/2), zoom=14)\n",
        "Map_ImgSeg.addLayer(usda_crop,{},name='USDA NASS Cropland 2022')\n",
        "Map_ImgSeg.addLayer(ndvi_s2_Img,{\"min\":0,\"max\":1},name='s2 NDVI_'+doi)\n",
        "add_img_vec(Map_ImgSeg, true_color_s2_Img, true_color_bands, true_color_sam_vector_path, true_color_s2_outfname, style)\n",
        "add_img_vec(Map_ImgSeg, false_color_s2_Img, false_color_bands, false_color_sam_vector_path, false_color_s2_outfname, style)\n",
        "Map_ImgSeg.addLayer(ee.Geometry.BBox(-93.930, 43.287, -93.921, 43.299), {'color':'red'}, opacity = 0.5)\n",
        "Map_ImgSeg.addLayer(ee.Geometry.BBox(-93.991, 43.308, -93.985, 43.312), {'color':'red'}, opacity = 0.5)\n",
        "Map_ImgSeg.addLayer(ee.Geometry.BBox(-93.972, 43.29, -93.931, 43.30), {'color':'red'}, opacity = 0.5)\n",
        "Map_ImgSeg.addLayerControl()\n",
        "Map_ImgSeg"
      ],
      "metadata": {
        "id": "1cbKfMPfWROz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By comparing the crop field boundaries delineated by SAM from S2 true-color and false-color images, we can see that they both generally agree with the boundaries shown on the USDA cropland data. This indicates that SAM can be used as a tool that quickly delineate the field boundaries.\n",
        "\n",
        "If we take a closer look, we can find that in some areas the field boundaries delineated from the false-color image better agree with the details shown on the NDVI image (e.g., the areas marked by red boxes).\n",
        "\n",
        "This indicates that SAM may be used to reveal the boundaries between the crops that have different health conditions when being applied to the false-color image. However, more examination on the delineated boundaries is required to reach a solid conclusion."
      ],
      "metadata": {
        "id": "h--CW2xZbdry"
      }
    }
  ]
}
